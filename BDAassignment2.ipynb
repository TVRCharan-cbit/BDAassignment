{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWlINGLqt3v3/xeAksNOwi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TVRCharan-cbit/BDAassignment/blob/main/BDAassignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build a Classification Model with Spark with a dataset of your choice"
      ],
      "metadata": {
        "id": "CFVX7zM6-pGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up PySpark in Colab"
      ],
      "metadata": {
        "id": "Uk2OukNl-yui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-11-jdk -qq > /dev/null\n",
        "!pip install -q pyspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"PlantSurvival\").getOrCreate()"
      ],
      "metadata": {
        "id": "4sQYKGo1-svO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a synthetic dataset"
      ],
      "metadata": {
        "id": "GwB07tmK_Nrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "data = [\n",
        "    Row(moisture=30, sunlight=5, temperature=15, survived=1),\n",
        "    Row(moisture=80, sunlight=9, temperature=35, survived=0),\n",
        "    Row(moisture=50, sunlight=6, temperature=22, survived=1),\n",
        "    Row(moisture=90, sunlight=4, temperature=18, survived=0),\n",
        "    Row(moisture=60, sunlight=8, temperature=28, survived=1),\n",
        "    Row(moisture=20, sunlight=3, temperature=10, survived=0),\n",
        "    Row(moisture=40, sunlight=7, temperature=20, survived=1),\n",
        "    Row(moisture=70, sunlight=2, temperature=12, survived=0),\n",
        "    Row(moisture=45, sunlight=6, temperature=21, survived=1),\n",
        "    Row(moisture=85, sunlight=10, temperature=33, survived=0),\n",
        "    Row(moisture=35, sunlight=5, temperature=17, survived=1),\n",
        "    Row(moisture=55, sunlight=4, temperature=19, survived=1),\n",
        "    Row(moisture=75, sunlight=3, temperature=14, survived=0),\n",
        "\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWv3Maqb-2fA",
        "outputId": "0b38eac8-7de1-4ade-fe2c-b1b712bff61b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+-----------+--------+\n",
            "|moisture|sunlight|temperature|survived|\n",
            "+--------+--------+-----------+--------+\n",
            "|      30|       5|         15|       1|\n",
            "|      80|       9|         35|       0|\n",
            "|      50|       6|         22|       1|\n",
            "|      90|       4|         18|       0|\n",
            "|      60|       8|         28|       1|\n",
            "|      20|       3|         10|       0|\n",
            "|      40|       7|         20|       1|\n",
            "|      70|       2|         12|       0|\n",
            "|      45|       6|         21|       1|\n",
            "|      85|      10|         33|       0|\n",
            "|      35|       5|         17|       1|\n",
            "|      55|       4|         19|       1|\n",
            "|      75|       3|         14|       0|\n",
            "+--------+--------+-----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "r6Pno5Dw_RnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"moisture\", \"sunlight\", \"temperature\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "assembled_df = assembler.transform(df).select(\"features\", \"survived\")\n",
        "assembled_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWaZsVha-8L4",
        "outputId": "8163a9b8-8e25-4fd5-ed5e-32fc9d2f6ddb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+--------+\n",
            "|        features|survived|\n",
            "+----------------+--------+\n",
            "| [30.0,5.0,15.0]|       1|\n",
            "| [80.0,9.0,35.0]|       0|\n",
            "| [50.0,6.0,22.0]|       1|\n",
            "| [90.0,4.0,18.0]|       0|\n",
            "| [60.0,8.0,28.0]|       1|\n",
            "| [20.0,3.0,10.0]|       0|\n",
            "| [40.0,7.0,20.0]|       1|\n",
            "| [70.0,2.0,12.0]|       0|\n",
            "| [45.0,6.0,21.0]|       1|\n",
            "|[85.0,10.0,33.0]|       0|\n",
            "| [35.0,5.0,17.0]|       1|\n",
            "| [55.0,4.0,19.0]|       1|\n",
            "| [75.0,3.0,14.0]|       0|\n",
            "+----------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = assembled_df.randomSplit([0.75, 0.25], seed=123)"
      ],
      "metadata": {
        "id": "7NVlKyXd-_v4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "dt = DecisionTreeClassifier(labelCol=\"survived\", featuresCol=\"features\")\n",
        "model = dt.fit(train)"
      ],
      "metadata": {
        "id": "yZsN5LNc_DAE"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.transform(test)\n",
        "predictions.select(\"features\", \"survived\", \"prediction\", \"probability\").show()\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"survived\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Decision Tree Test Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYaM_SIk_GAw",
        "outputId": "5b5beba2-3fbf-4f55-d37f-9542b22b5d03"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------+----------+-----------+\n",
            "|       features|survived|prediction|probability|\n",
            "+---------------+--------+----------+-----------+\n",
            "|[50.0,6.0,22.0]|       1|       1.0|  [0.0,1.0]|\n",
            "|[45.0,6.0,21.0]|       1|       1.0|  [0.0,1.0]|\n",
            "+---------------+--------+----------+-----------+\n",
            "\n",
            "Decision Tree Test Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build a Clustering Model with Spark with a dataset of your choice"
      ],
      "metadata": {
        "id": "JdF3porp_J2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"CustomerClustering\").getOrCreate()"
      ],
      "metadata": {
        "id": "LQvnF9muASRX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "data = [\n",
        "    Row(food=120, electronics=300, entertainment=200),\n",
        "    Row(food=80, electronics=500, entertainment=150),\n",
        "    Row(food=200, electronics=150, entertainment=100),\n",
        "    Row(food=300, electronics=80, entertainment=120),\n",
        "    Row(food=400, electronics=100, entertainment=200),\n",
        "    Row(food=100, electronics=800, entertainment=300),\n",
        "    Row(food=50, electronics=900, entertainment=250),\n",
        "    Row(food=280, electronics=150, entertainment=100),\n",
        "    Row(food=320, electronics=120, entertainment=80),\n",
        "    Row(food=70, electronics=850, entertainment=260),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuoB90NeAWLr",
        "outputId": "137550b6-0ee1-45ec-f513-d63d11ccb8bb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+-----------+-------------+\n",
            "|food|electronics|entertainment|\n",
            "+----+-----------+-------------+\n",
            "| 120|        300|          200|\n",
            "|  80|        500|          150|\n",
            "| 200|        150|          100|\n",
            "| 300|         80|          120|\n",
            "| 400|        100|          200|\n",
            "| 100|        800|          300|\n",
            "|  50|        900|          250|\n",
            "| 280|        150|          100|\n",
            "| 320|        120|           80|\n",
            "|  70|        850|          260|\n",
            "+----+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"food\", \"electronics\", \"entertainment\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "assembled_df = assembler.transform(df).select(\"features\")\n",
        "assembled_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zNKu5RxAXNk",
        "outputId": "4993065b-0b9b-4035-da13-40a0f61ac768"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|           features|\n",
            "+-------------------+\n",
            "|[120.0,300.0,200.0]|\n",
            "| [80.0,500.0,150.0]|\n",
            "|[200.0,150.0,100.0]|\n",
            "| [300.0,80.0,120.0]|\n",
            "|[400.0,100.0,200.0]|\n",
            "|[100.0,800.0,300.0]|\n",
            "| [50.0,900.0,250.0]|\n",
            "|[280.0,150.0,100.0]|\n",
            "| [320.0,120.0,80.0]|\n",
            "| [70.0,850.0,260.0]|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "kmeans = KMeans(featuresCol=\"features\", k=2, seed=1)  # Try k=3 later too!\n",
        "model = kmeans.fit(assembled_df)\n",
        "clusters = model.transform(assembled_df)\n",
        "clusters.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpW47PRDAb_u",
        "outputId": "bd826223-407c-4bb5-be0e-1b01f33cf8b6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+----------+\n",
            "|           features|prediction|\n",
            "+-------------------+----------+\n",
            "|[120.0,300.0,200.0]|         0|\n",
            "| [80.0,500.0,150.0]|         0|\n",
            "|[200.0,150.0,100.0]|         0|\n",
            "| [300.0,80.0,120.0]|         0|\n",
            "|[400.0,100.0,200.0]|         0|\n",
            "|[100.0,800.0,300.0]|         1|\n",
            "| [50.0,900.0,250.0]|         1|\n",
            "|[280.0,150.0,100.0]|         0|\n",
            "| [320.0,120.0,80.0]|         0|\n",
            "| [70.0,850.0,260.0]|         1|\n",
            "+-------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "centers = model.clusterCenters()\n",
        "print(\"Cluster Centers:\")\n",
        "for idx, center in enumerate(centers):\n",
        "    print(f\"Cluster {idx}: {center}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGV5IDeHAf_-",
        "outputId": "1892bbf3-956c-4a49-ea81-13facc8f0bf0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster Centers:\n",
            "Cluster 0: [242.85714286 200.         135.71428571]\n",
            "Cluster 1: [ 73.33333333 850.         270.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Build a Recommendation Engine with Spark with a dataset of your\n"
      ],
      "metadata": {
        "id": "jDiPvkYbA6u-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"MovieRecommender\").getOrCreate()"
      ],
      "metadata": {
        "id": "98mCxn9hAj5r"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "ratings_data = [\n",
        "    Row(user_id=1, movie_id=101, rating=4.0),\n",
        "    Row(user_id=1, movie_id=102, rating=3.5),\n",
        "    Row(user_id=1, movie_id=103, rating=2.0),\n",
        "    Row(user_id=2, movie_id=101, rating=5.0),\n",
        "    Row(user_id=2, movie_id=104, rating=3.0),\n",
        "    Row(user_id=2, movie_id=105, rating=4.5),\n",
        "    Row(user_id=3, movie_id=102, rating=4.0),\n",
        "    Row(user_id=3, movie_id=103, rating=3.0),\n",
        "    Row(user_id=3, movie_id=105, rating=4.0),\n",
        "    Row(user_id=4, movie_id=101, rating=2.5),\n",
        "    Row(user_id=4, movie_id=104, rating=4.5),\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(ratings_data)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8G1jQDvA5s3",
        "outputId": "6336baae-30d9-46aa-87ab-b1fcd12b349b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+------+\n",
            "|user_id|movie_id|rating|\n",
            "+-------+--------+------+\n",
            "|      1|     101|   4.0|\n",
            "|      1|     102|   3.5|\n",
            "|      1|     103|   2.0|\n",
            "|      2|     101|   5.0|\n",
            "|      2|     104|   3.0|\n",
            "|      2|     105|   4.5|\n",
            "|      3|     102|   4.0|\n",
            "|      3|     103|   3.0|\n",
            "|      3|     105|   4.0|\n",
            "|      4|     101|   2.5|\n",
            "|      4|     104|   4.5|\n",
            "+-------+--------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.recommendation import ALS\n",
        "\n",
        "als = ALS(\n",
        "    userCol=\"user_id\",\n",
        "    itemCol=\"movie_id\",\n",
        "    ratingCol=\"rating\",\n",
        "    coldStartStrategy=\"drop\",\n",
        "    nonnegative=True,\n",
        "    rank=10,\n",
        "    maxIter=10,\n",
        "    regParam=0.1\n",
        ")\n",
        "\n",
        "model = als.fit(df)"
      ],
      "metadata": {
        "id": "XwBgVekoBDuN"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.transform(df)\n",
        "predictions.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2Fq6iSHBHTq",
        "outputId": "05cae77c-4a1e-4823-b391-dd11b2ce2e7b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+------+----------+\n",
            "|user_id|movie_id|rating|prediction|\n",
            "+-------+--------+------+----------+\n",
            "|      1|     101|   4.0| 3.8921983|\n",
            "|      1|     102|   3.5|   3.40597|\n",
            "|      1|     103|   2.0| 2.0751464|\n",
            "|      2|     101|   5.0|  4.821928|\n",
            "|      2|     104|   3.0| 3.0272722|\n",
            "|      3|     102|   4.0|  3.948411|\n",
            "|      3|     103|   3.0|  2.816791|\n",
            "|      3|     105|   4.0| 3.9935288|\n",
            "|      4|     101|   2.5|  2.545436|\n",
            "|      4|     104|   4.5|   4.34013|\n",
            "|      2|     105|   4.5| 4.4708824|\n",
            "+-------+--------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_recs = model.recommendForAllUsers(3)\n",
        "user_recs.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE-Bu4yTBLL6",
        "outputId": "a3c56970-37d3-4623-9fe8-cff14d4fd4e7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------------------------------------------------+\n",
            "|user_id|recommendations                                      |\n",
            "+-------+-----------------------------------------------------+\n",
            "|1      |[{101, 3.8921983}, {105, 3.549892}, {102, 3.40597}]  |\n",
            "|2      |[{101, 4.821928}, {105, 4.4708824}, {102, 4.269059}] |\n",
            "|3      |[{105, 3.9935286}, {102, 3.948411}, {101, 3.8892796}]|\n",
            "|4      |[{104, 4.34013}, {101, 2.545436}, {102, 2.3206692}]  |\n",
            "+-------+-----------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ob8FqGfQBPSw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}